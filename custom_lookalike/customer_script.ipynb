{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERIZED VARIABLES\n",
    "\n",
    "PROJECT_ID = \"{{PROJECT_ID}}\"\n",
    "TARGET_BUCKET = \"{{TARGET_BUCKET}}\"\n",
    "ORGANIZATION_ID = \"{{ORGANIZATION_ID}}\"\n",
    "BUCKET_PIPELINE = \"{{BUCKET_PIPELINE}}\"\n",
    "DATAFOUNDATION_TABLE = \"{{DATAFOUNDATION_TABLE}}\"\n",
    "MODEL_URI = \"{{MODEL_URI}}\"\n",
    "\n",
    "\n",
    "SCALER_URI = \"{{SCALER_URI}}\"\n",
    "AUDIENCE_ID = \"{{AUDIENCE_ID}}\"\n",
    "TOPIC_ID = \"{{TOPIC_ID}}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PARAMETERIZED VARIABLES\n",
    "\n",
    "# PROJECT_ID = \"sbox-ext-collab-prd-50f1\"\n",
    "# TARGET_BUCKET = \"gs://model-results-lookalike-model-sandbox\"\n",
    "# ORGANIZATION_ID = \"CUSTOMER_A\"\n",
    "# BUCKET_PIPELINE = \"gs://bucket-collab-prd/Arifian/vertex_pipeline_files\"\n",
    "# DATAFOUNDATION_TABLE = \"owned_summary.df_customer_data_profile\"\n",
    "# MODEL_URI = \"gs://bucket-collab-prd/Arifian/vertex_pipeline_files/pickle_files/knn.pkl\"\n",
    "\n",
    "\n",
    "# SCALER_URI = \"gs://bucket-collab-prd/Arifian/vertex_pipeline_files/pickle_files/scaler.pkl\"\n",
    "# AUDIENCE_ID = 1\n",
    "# TOPIC_ID = \"tempbucket-to-audience-table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = 'custom_lookalike_'+ORGANIZATION_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TARGET_GCS_LOCATION = TARGET_BUCKET+\"/\"+ORGANIZATION_ID\n",
    "REGION = \"asia-southeast2\"\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "PIPELINE_ROOT = f\"{BUCKET_PIPELINE}/pipeline_root/custom_lookalike_{ORGANIZATION_ID}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "from typing import NamedTuple\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_PIPELINE, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "tmpdir = tempfile.gettempdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query from BQ\n",
    "\n",
    "@component(\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=[\n",
    "        \"pandas==1.5.2\", \"google-cloud-bigquery==2.34.4\", \"pyarrow==10.0.1\"\n",
    "    ],\n",
    "    output_component_file=tmpdir+\"/query_from_bq.yaml\"\n",
    ")\n",
    "def query_from_bq(\n",
    "    data: Output[Dataset], table_full_name: str,\n",
    "    project_id: str\n",
    "):\n",
    "    from google.cloud import bigquery\n",
    "    client = bigquery.Client(project_id)\n",
    "#     sql = f\"\"\"SELECT \n",
    "#     *\n",
    "# FROM `{table_full_name}` \n",
    "# WHERE (partition_month = '2022-10-01')\"\"\"\n",
    "    sql = f\"\"\"SELECT \n",
    "    msisdn,\n",
    "    total_arpu, data_usage_in_mb, \n",
    "    data_usage_duration, \n",
    "    total_topups, \n",
    "    ARPU_1m, \n",
    "    number_of_topups_1m, \n",
    "    total_topups_1m, \n",
    "    total_usage_GB_1m, \n",
    "    daily_GB_consumption_rate_1m,\n",
    "    number_of_topups_2m, \n",
    "    total_topups_2m, \n",
    "    total_usage_GB_2m\n",
    "FROM `{table_full_name}` \n",
    "WHERE (partition_month = '2022-10-01')\n",
    "\"\"\"\n",
    "    df = client.query(sql).to_dataframe()\n",
    "    print('data stored to df')\n",
    "\n",
    "    df.to_parquet(data.path, index=False)\n",
    "    print('data stored to parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "\n",
    "@component(\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=[\n",
    "        \"pandas==1.5.2\", \"pyarrow==10.0.1\", \"scikit-learn==1.1.3\", \"joblib==1.2.0\", \"gcsfs==2022.11.0\"\n",
    "    ],\n",
    "    output_component_file=tmpdir+\"/preprocess.yaml\"\n",
    ")\n",
    "def preprocess(\n",
    "    data: Input[Dataset], preprocessed_data: Output[Dataset],\n",
    "    scaler_uri: str = None\n",
    "):\n",
    "    import pandas as pd\n",
    "    import gcsfs, joblib\n",
    "\n",
    "    df_data = pd.read_parquet(data.path)\n",
    "    df_preprocessed = df_data[[\n",
    "        'msisdn',\n",
    "        'total_arpu', 'data_usage_in_mb', \n",
    "        'data_usage_duration', \n",
    "        'total_topups', \n",
    "        'ARPU_1m', \n",
    "        'number_of_topups_1m', \n",
    "        'total_topups_1m', \n",
    "        'total_usage_GB_1m', \n",
    "        'daily_GB_consumption_rate_1m',\n",
    "        'number_of_topups_2m', \n",
    "        'total_topups_2m', \n",
    "        'total_usage_GB_2m'\n",
    "    ]]\n",
    "    df_preprocessed.fillna(0, inplace=True)\n",
    "\n",
    "    if scaler_uri:\n",
    "        fs = gcsfs.GCSFileSystem()\n",
    "        with fs.open(scaler_uri, \"rb\") as f:\n",
    "            scaler = joblib.load(f)  \n",
    "    \n",
    "        X_test_scaled = scaler.transform(df_preprocessed[[x for x in df_preprocessed.columns if x not in ['msisdn']]])\n",
    "        df_preprocessed = pd.DataFrame(X_test_scaled, columns = df_preprocessed[[x for x in df_preprocessed.columns if x not in ['msisdn']]].columns)\n",
    "\n",
    "    df_preprocessed.to_parquet(preprocessed_data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model inference\n",
    "\n",
    "@component(\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=[\n",
    "        \"pandas==1.5.2\", \"pyarrow==10.0.1\", \"scikit-learn==1.1.3\", \"joblib==1.2.0\", \"gcsfs==2022.11.0\"\n",
    "    ],\n",
    "    output_component_file=tmpdir+\"/predict.yaml\"\n",
    ")\n",
    "def predict(\n",
    "    model_uri: str, preprocessed_data: Input[Dataset], \n",
    "    original_data: Input[Dataset],\n",
    "    prediction_result: Output[Dataset]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import gcsfs, joblib\n",
    "\n",
    "    df_preprocessed = pd.read_parquet(preprocessed_data.path)\n",
    "    df_original = pd.read_parquet(original_data.path)\n",
    "\n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "    with fs.open(model_uri, \"rb\") as f:\n",
    "        model = joblib.load(f)\n",
    "\n",
    "    score = model.predict(df_preprocessed)\n",
    "    df_original['score'] = score\n",
    "\n",
    "    df_result = df_original[df_original['score']==1][['msisdn']]\n",
    "\n",
    "    df_result.to_parquet(prediction_result.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to gcs\n",
    "\n",
    "@component(\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=[\n",
    "        \"pandas==1.5.2\", \"gcsfs==2022.11.0\", \"pyarrow==10.0.1\", \"datetime\"\n",
    "    ],\n",
    "    output_component_file=tmpdir+\"/write_to_gcs.yaml\"\n",
    ")\n",
    "def write_to_gcs(\n",
    "    data: Input[Dataset], project_id: str, \n",
    "    audience_id: int, bucket: str\n",
    ") -> NamedTuple('Outputs', [('data_gcs_path', str)]):\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    df_to_store = pd.read_parquet(data.path)\n",
    "    df_to_store['audience_id'] = audience_id\n",
    "    df_to_store = df_to_store[['audience_id', 'msisdn']]\n",
    "\n",
    "    path = bucket+f\"/custom_lookalike_{TIMESTAMP}.csv\"\n",
    "\n",
    "    df_to_store.to_csv(path, index=False, header=False)\n",
    "    from typing import NamedTuple\n",
    "    \n",
    "    outputs = NamedTuple('Outputs', [('data_gcs_path', str)])\n",
    "    return outputs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# publish to pubsub\n",
    "\n",
    "@component(\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-pubsub==2.13.4\", \"datetime\"\n",
    "    ],\n",
    "    output_component_file=tmpdir+\"/aud_data_to_pubsub.yaml\"\n",
    ")\n",
    "\n",
    "def aud_data_to_pubsub(\n",
    "    data_gcs_path: str, topic_id: str,\n",
    "    project_id: str, audience_id: int\n",
    "):\n",
    "    import json\n",
    "    from google.cloud import pubsub_v1\n",
    "    publisher = pubsub_v1.PublisherClient()\n",
    "    topic_path = publisher.topic_path(project_id, topic_id)\n",
    "\n",
    "    message = {\n",
    "        'audience_id': audience_id,\n",
    "        'file_path': data_gcs_path,\n",
    "    }\n",
    "\n",
    "    future = publisher.publish(topic_path, json.dumps(message).encode(\"utf-8\"))\n",
    "    print(future.result())\n",
    "    print(f\"Published messages to {topic_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_cloud_pipeline_components.v1.custom_job import create_custom_training_job_from_component\n",
    "\n",
    "query_from_bq_v2 = create_custom_training_job_from_component(\n",
    "    query_from_bq,\n",
    "    display_name = 'query_from_bq',\n",
    "    machine_type = 'n1-highmem-16',\n",
    ")\n",
    "\n",
    "preprocess_v2 = create_custom_training_job_from_component(\n",
    "    preprocess,\n",
    "    display_name = 'preprocess',\n",
    "    machine_type = 'n1-highmem-16',\n",
    ")\n",
    "\n",
    "predict_v2 = create_custom_training_job_from_component(\n",
    "    predict,\n",
    "    display_name = 'predict',\n",
    "    machine_type = 'n1-highmem-16',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    name=pipeline_name.replace(\"_\",\"-\").lower(),\n",
    ")\n",
    "def pipeline(\n",
    "    model_uri: str,\n",
    "    target_bucket: str,\n",
    "    datafoundation_table_fullname: str,\n",
    "    project_id: str,\n",
    "    topic_id: str,\n",
    "    audience_id: int,\n",
    "    data_gcs_path: str,\n",
    "    scaler_uri: str = None\n",
    "):\n",
    "    query_audience_data = query_from_bq_v2(\n",
    "        table_full_name=datafoundation_table_fullname,\n",
    "        project_id=project_id,\n",
    "        project=project_id,\n",
    "        location=REGION\n",
    "    )\n",
    "    preprocess_audience_data = preprocess_v2(\n",
    "        data=query_audience_data.outputs[\"data\"],\n",
    "        scaler_uri=scaler_uri,\n",
    "        project=project_id,\n",
    "        location=REGION\n",
    "    )\n",
    "    custom_lookalike_predict = predict_v2(\n",
    "        model_uri=model_uri,\n",
    "        preprocessed_data=preprocess_audience_data.outputs[\"preprocessed_data\"],\n",
    "        original_data=query_audience_data.outputs[\"data\"],\n",
    "        project=project_id,\n",
    "        location=REGION\n",
    "    )    \n",
    "    write_result_to_gcs = write_to_gcs(\n",
    "        data=custom_lookalike_predict.outputs[\"prediction_result\"],\n",
    "        bucket=target_bucket, project_id=project_id,\n",
    "        audience_id=audience_id\n",
    "    )\n",
    "    trigger_load_gcs_to_cloudsql = aud_data_to_pubsub(\n",
    "        data_gcs_path=write_result_to_gcs.outputs['data_gcs_path'],\n",
    "        topic_id=topic_id, project_id=project_id, \n",
    "        audience_id=audience_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ananda.dwi/Documents/indosat/poc_vertex/env/lib/python3.9/site-packages/kfp/v2/compiler/compiler.py:1290: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from kfp.v2 import compiler\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=tmpdir+f\"/{pipeline_name}.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/731696491468/locations/asia-southeast2/pipelineJobs/custom-lookalike-customer-a-20221220205916\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/731696491468/locations/asia-southeast2/pipelineJobs/custom-lookalike-customer-a-20221220205916')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/asia-southeast2/pipelines/runs/custom-lookalike-customer-a-20221220205916?project=731696491468\n",
      "PipelineJob projects/731696491468/locations/asia-southeast2/pipelineJobs/custom-lookalike-customer-a-20221220205916 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/731696491468/locations/asia-southeast2/pipelineJobs/custom-lookalike-customer-a-20221220205916 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/731696491468/locations/asia-southeast2/pipelineJobs/custom-lookalike-customer-a-20221220205916 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/731696491468/locations/asia-southeast2/pipelineJobs/custom-lookalike-customer-a-20221220205916 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "DISPLAY_NAME = pipeline_name + \"_\" + TIMESTAMP\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=tmpdir+f\"/{pipeline_name}.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\n",
    "        \"project_id\": PROJECT_ID,\n",
    "        \"target_bucket\": TARGET_GCS_LOCATION,\n",
    "        \"model_uri\": MODEL_URI,\n",
    "        \"scaler_uri\": None if SCALER_URI == 'None' else SCALER_URI,\n",
    "        \"datafoundation_table_fullname\": DATAFOUNDATION_TABLE,\n",
    "        \"audience_id\": AUDIENCE_ID,\n",
    "        \"topic_id\": TOPIC_ID,\n",
    "        \"data_gcs_path\": TARGET_GCS_LOCATION+f\"/custom_lookalike_{TIMESTAMP}.csv\"\n",
    "    },\n",
    "    enable_caching=False\n",
    ")\n",
    "job.run(sync=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "18ab93e5cae76da871c8ec2ed964d533dd2ee52f4436e759813a11e0df5d2609"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
