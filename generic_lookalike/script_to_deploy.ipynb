{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERIZED VARIABLES\n",
    "PROJECT_ID = \"project-for-sda-development\"\n",
    "PYSPARK_SCRIPT = \"gs://vertex-testing-poc-123/generic_lookalike/pyspark_script.py\"\n",
    "CUSTOM_IMAGE_TAG = \"asia-southeast2-docker.pkg.dev/project-for-sda-development/dataproc-image/my-image:1.0.1\"\n",
    "PARAMETERS = {\n",
    "    \"source_table\": \"project-for-sda-development.dataset_test.audience_table\",\n",
    "    \"target_table\": \"project-for-sda-development.dataset_test.generic_lookalike_test\",\n",
    "    \"feature_list\": \"total_arpu,data_usage_in_mb,total_topups\",\n",
    "    \"msisdn_list\": \"1,2,3,4,5,6,7,8,9,10,27\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "BUCKET_TEMP = \"vertex-testing-poc-123/generic_lookalike/temp\"\n",
    "SERVICE_ACCOUNT = \"serviceaccountnyavertex@project-for-sda-development.iam.gserviceaccount.com\"\n",
    "REGION = \"asia-southeast2\"\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET_NAME = \"vertex-testing-poc-123\"\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/generic_pyspark\".format(BUCKET_URI)\n",
    "BATCH_ID = \"generic-pyspark-\" + TIMESTAMP\n",
    "SUBNETWORK_URI = \"projects/project-for-sda-development/regions/asia-southeast2/subnetworks/default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_ARG = []\n",
    "for key, value in PARAMETERS.items():\n",
    "    SCRIPT_ARG.append(f\"--{key}\")\n",
    "    SCRIPT_ARG.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/var/folders/s4/8_vd44gd01569g443f_jqg1h0000gn/T'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tempfile\n",
    "tmpdir = tempfile.gettempdir()\n",
    "tmpdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ananda.dwi/Documents/indosat/poc_vertex/env/lib/python3.9/site-packages/kfp/v2/compiler/compiler.py:1290: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/960061420307/locations/asia-southeast2/pipelineJobs/generic-lookalike-pyspark-20221215155649\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/960061420307/locations/asia-southeast2/pipelineJobs/generic-lookalike-pyspark-20221215155649')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/asia-southeast2/pipelines/runs/generic-lookalike-pyspark-20221215155649?project=960061420307\n",
      "PipelineJob projects/960061420307/locations/asia-southeast2/pipelineJobs/generic-lookalike-pyspark-20221215155649 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/960061420307/locations/asia-southeast2/pipelineJobs/generic-lookalike-pyspark-20221215155649 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/960061420307/locations/asia-southeast2/pipelineJobs/generic-lookalike-pyspark-20221215155649 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/960061420307/locations/asia-southeast2/pipelineJobs/generic-lookalike-pyspark-20221215155649 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/960061420307/locations/asia-southeast2/pipelineJobs/generic-lookalike-pyspark-20221215155649 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/960061420307/locations/asia-southeast2/pipelineJobs/generic-lookalike-pyspark-20221215155649 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/960061420307/locations/asia-southeast2/pipelineJobs/generic-lookalike-pyspark-20221215155649\n"
     ]
    }
   ],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"generic-lookalike-pyspark\",\n",
    ")\n",
    "def pipeline(\n",
    "    main_python_file_uri: str,\n",
    "    args: list,\n",
    "    project_id: str,\n",
    "    container_image: str,\n",
    "    batch_id: str = BATCH_ID,\n",
    "    location: str = REGION,   \n",
    "    service_account: str = SERVICE_ACCOUNT, \n",
    "    bucket: str = BUCKET_TEMP, \n",
    "):\n",
    "    from google_cloud_pipeline_components.v1.dataproc import \\\n",
    "        DataprocPySparkBatchOp\n",
    "\n",
    "    _ = DataprocPySparkBatchOp(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        main_python_file_uri=main_python_file_uri,\n",
    "        service_account=service_account,\n",
    "        args=args,\n",
    "        batch_id=batch_id,\n",
    "        container_image=container_image,\n",
    "        subnetwork_uri=SUBNETWORK_URI,\n",
    "    )\n",
    "\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=f\"{tmpdir}/pipeline.json\")\n",
    "\n",
    "pipeline = aiplatform.PipelineJob(\n",
    "    display_name=\"generic-lookalike-pyspark\",\n",
    "    template_path=f\"{tmpdir}/pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching=False,\n",
    "    parameter_values={\n",
    "        \"main_python_file_uri\": PYSPARK_SCRIPT,\n",
    "        \"args\": SCRIPT_ARG,\n",
    "        \"project_id\": PROJECT_ID,\n",
    "        \"container_image\": CUSTOM_IMAGE_TAG\n",
    "    }\n",
    ")\n",
    "\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "18ab93e5cae76da871c8ec2ed964d533dd2ee52f4436e759813a11e0df5d2609"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
