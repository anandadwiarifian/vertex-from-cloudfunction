{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERIZED VARIABLES\n",
    "\n",
    "PROJECT_ID = \"{{PROJECT_ID}}\"\n",
    "TARGET_TABLE = \"{{TARGET_TABLE}}\"\n",
    "MODEL_URI = \"{{MODEL_URI}}\"\n",
    "SCALER_URI = \"{{SCALER_URI}}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "AUDIENCE_TABLE = \"dataset_test.audience_table\"\n",
    "REGION = \"asia-southeast2\"\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET_NAME = \"vertex-testing-poc-123\"\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/creditscoring_test\".format(BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_URI, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "tmpdir = tempfile.gettempdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query from BQ\n",
    "\n",
    "@component(\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=[\n",
    "        \"pandas==1.5.2\", \"google-cloud-bigquery==2.34.4\", \"pyarrow==10.0.1\"\n",
    "    ],\n",
    "    output_component_file=tmpdir+\"/query_from_bq.yaml\"\n",
    ")\n",
    "def query_from_bq(\n",
    "    data: Output[Dataset], table_full_name: str,\n",
    "    project_id: str\n",
    "):\n",
    "    from google.cloud import bigquery\n",
    "    client = bigquery.Client(project_id)\n",
    "    sql = f\"\"\"SELECT \n",
    "    *\n",
    "FROM `{table_full_name}` \n",
    "WHERE (partition_month = '2022-10-01')\"\"\"\n",
    "    df = client.query(sql).to_dataframe()\n",
    "\n",
    "    df.to_parquet(data.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "\n",
    "@component(\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=[\n",
    "        \"pandas==1.5.2\", \"pyarrow==10.0.1\", \"scikit-learn==1.1.3\", \"joblib==1.2.0\", \"gcsfs==2022.11.0\"\n",
    "    ],\n",
    "    output_component_file=tmpdir+\"/preprocess.yaml\"\n",
    ")\n",
    "def preprocess(\n",
    "    data: Input[Dataset], preprocessed_data: Output[Dataset],\n",
    "    scaler_uri: str = None\n",
    "):\n",
    "    import pandas as pd\n",
    "    import gcsfs, joblib\n",
    "\n",
    "    df_data = pd.read_parquet(data.path)\n",
    "    df_preprocessed = df_data[[\n",
    "        'msisdn',\n",
    "        'total_arpu', 'data_usage_in_mb', \n",
    "        'data_usage_duration', \n",
    "        'total_topups', \n",
    "        'ARPU_1m', \n",
    "        'number_of_topups_1m', \n",
    "        'total_topups_1m', \n",
    "        'total_usage_GB_1m', \n",
    "        'daily_GB_consumption_rate_1m',\n",
    "        'number_of_topups_2m', \n",
    "        'total_topups_2m', \n",
    "        'total_usage_GB_2m'\n",
    "    ]]\n",
    "    df_preprocessed.fillna(0, inplace=True)\n",
    "\n",
    "    if scaler_uri:\n",
    "        fs = gcsfs.GCSFileSystem()\n",
    "        with fs.open(scaler_uri, \"rb\") as f:\n",
    "            scaler = joblib.load(f)  \n",
    "    \n",
    "        X_test_scaled = scaler.transform(df_preprocessed[[x for x in df_preprocessed.columns if x not in ['msisdn']]])\n",
    "        df_preprocessed = pd.DataFrame(X_test_scaled, columns = df_preprocessed[[x for x in df_preprocessed.columns if x not in ['msisdn']]].columns)\n",
    "\n",
    "    df_preprocessed.to_parquet(preprocessed_data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model inference\n",
    "\n",
    "@component(\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=[\n",
    "        \"pandas==1.5.2\", \"pyarrow==10.0.1\", \"scikit-learn==1.1.3\", \"joblib==1.2.0\", \"gcsfs==2022.11.0\"\n",
    "    ],\n",
    "    output_component_file=tmpdir+\"/predict.yaml\"\n",
    ")\n",
    "def predict(\n",
    "    model_uri: str, preprocessed_data: Input[Dataset], \n",
    "    original_data: Input[Dataset],\n",
    "    prediction_result: Output[Dataset]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import gcsfs, joblib\n",
    "\n",
    "    df_preprocessed = pd.read_parquet(preprocessed_data.path)\n",
    "    df_original = pd.read_parquet(original_data.path)\n",
    "\n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "    with fs.open(model_uri, \"rb\") as f:\n",
    "        model = joblib.load(f)\n",
    "\n",
    "    score = model.predict(df_preprocessed)\n",
    "    df_original['score'] = score\n",
    "\n",
    "    df_original.to_parquet(prediction_result.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to BQ\n",
    "\n",
    "@component(\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=[\n",
    "        \"pandas==1.5.2\", \"google-cloud-bigquery==2.34.4\", \"pyarrow==10.0.1\", \"datetime\"\n",
    "    ],\n",
    "    output_component_file=tmpdir+\"/write_to_bq.yaml\"\n",
    ")\n",
    "def write_to_bq(\n",
    "    data: Input[Dataset], table_full_name: str,\n",
    "    project_id: str, write_mode: str\n",
    "):\n",
    "    import pandas as pd\n",
    "    from google.cloud import bigquery\n",
    "    client = bigquery.Client(project_id)\n",
    "\n",
    "    df_to_store = pd.read_parquet(data.path)\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        autodetect=True,\n",
    "        write_disposition=write_mode,\n",
    "    )\n",
    "\n",
    "    job = client.load_table_from_dataframe(\n",
    "        df_to_store,\n",
    "        table_full_name,\n",
    "        job_config=job_config\n",
    "    )  # Make an API request.\n",
    "    job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    name=\"creditscoring-sample\",\n",
    ")\n",
    "def pipeline(\n",
    "    model_uri: str,\n",
    "    target_table_fullname: str,\n",
    "    audience_table_fullname: str,\n",
    "    project_id: str,\n",
    "    scaler_uri: str = None,\n",
    "    write_mode: str = \"WRITE_APPEND\"\n",
    "):\n",
    "    query_audience_data = query_from_bq(\n",
    "        table_full_name=audience_table_fullname,\n",
    "        project_id=project_id\n",
    "    )\n",
    "    preprocess_audience_data = preprocess(\n",
    "        data=query_audience_data.outputs[\"data\"],\n",
    "        scaler_uri=scaler_uri\n",
    "    )\n",
    "    predict_creditscoring = predict(\n",
    "        model_uri=model_uri,\n",
    "        preprocessed_data=preprocess_audience_data.outputs[\"preprocessed_data\"],\n",
    "        original_data=query_audience_data.outputs[\"data\"]\n",
    "    )\n",
    "    write_result_to_bq = write_to_bq(\n",
    "        data=predict_creditscoring.outputs[\"prediction_result\"],\n",
    "        table_full_name=target_table_fullname,\n",
    "        project_id=project_id,\n",
    "        write_mode=write_mode\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=tmpdir+\"/creditscoring_sample_pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/960061420307/locations/asia-southeast2/pipelineJobs/creditscoring-sample-20221209072212\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/960061420307/locations/asia-southeast2/pipelineJobs/creditscoring-sample-20221209072212')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/asia-southeast2/pipelines/runs/creditscoring-sample-20221209072212?project=960061420307\n",
      "PipelineJob projects/960061420307/locations/asia-southeast2/pipelineJobs/creditscoring-sample-20221209072212 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/960061420307/locations/asia-southeast2/pipelineJobs/creditscoring-sample-20221209072212 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/960061420307/locations/asia-southeast2/pipelineJobs/creditscoring-sample-20221209072212 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/960061420307/locations/asia-southeast2/pipelineJobs/creditscoring-sample-20221209072212 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/960061420307/locations/asia-southeast2/pipelineJobs/creditscoring-sample-20221209072212 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/960061420307/locations/asia-southeast2/pipelineJobs/creditscoring-sample-20221209072212\n"
     ]
    }
   ],
   "source": [
    "DISPLAY_NAME = \"creditscoring_test_\" + TIMESTAMP\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=tmpdir+\"/creditscoring_sample_pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\n",
    "        \"project_id\": PROJECT_ID,\n",
    "        \"target_table_fullname\": TARGET_TABLE,\n",
    "        \"model_uri\": MODEL_URI,\n",
    "        \"scaler_uri\": None if SCALER_URI == 'None' else SCALER_URI,\n",
    "        \"audience_table_fullname\": AUDIENCE_TABLE,\n",
    "        \"write_mode\": \"WRITE_TRUNCATE\"\n",
    "    }\n",
    ")\n",
    "job.run(sync=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "18ab93e5cae76da871c8ec2ed964d533dd2ee52f4436e759813a11e0df5d2609"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
